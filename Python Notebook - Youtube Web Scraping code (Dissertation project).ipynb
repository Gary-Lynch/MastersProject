{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a62b233",
   "metadata": {},
   "source": [
    "# Masters dissertation project: Gary Lynch (Part 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b95202",
   "metadata": {},
   "source": [
    "This Python notebook details the data processing and analysis completed as part of the MSC in Social Data Analytics at UCD. The author of this document is Gary Lynch - if you have any questions related to the analysis detailed below, you can reach me via email at gary.lynch1@ucdconnect.ie.\n",
    "\n",
    "This is the 2nd notebook of this project - part 1 shows how the data was loaded, cleaned, explored and the initial analysis. This notebook is purely focused on the webscraping needed for this project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6475e3a5",
   "metadata": {},
   "source": [
    "## Web scraping (Youtube)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a628a855",
   "metadata": {},
   "source": [
    "This section uses Youtube links that were shared in tweets about Brexit. The process for extracting those tweets is detailed in part 1. The first step is to load in our link data, created in part 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "596405ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in our link data from the csv files\n",
    "linkdata = pd.read_csv('C:/Users/Gary/Documents/Thesis Files/CSV STORE/AntiBrexit_Youtube_LINKS.csv')\n",
    "\n",
    "prolinkdata = pd.read_csv('C:/Users/Gary/Documents/Thesis Files/CSV STORE/ProBrexit_Youtube_LINKS.csv')\n",
    "\n",
    "#Setup empty stores for the results of our webscraping\n",
    "prostoredata = []\n",
    "storedata = []\n",
    "\n",
    "#Make the datasets into single column format.\n",
    "prolinkdata2 = prolinkdata[\"Link\"]\n",
    "linkdata2 = linkdata[\"Link\"]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f77da26",
   "metadata": {},
   "source": [
    "The next pieces of code show how to send calls to the list of Youtube links, parse the results, and save a specific element. In this case, the element is the channel name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "e27836ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing requests package (and other neccesary libraries)\n",
    "import requests  \n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Looping over the Youtube links\n",
    "for i in linkdata2:\n",
    "\n",
    "    # New get requests for each link\n",
    "    page = requests.get(i)\n",
    "    \n",
    "    # Create a soup to parse the resuls from\n",
    "    Soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    \n",
    "    # Append the name attribute to the storedata dataset\n",
    "    storedata.append(Soup.find_all(\"link\", itemprop=\"name\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f456b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the stored channel name data in a csv format\n",
    "\n",
    "storedata = pd.DataFrame(storedata)\n",
    "\n",
    "storedata.to_csv('C:/Users/Gary/Documents/Thesis Files/CSV STORE/anti_unpacked_Youtube_links.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "a13cf3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Looping over the Youtube links\n",
    "for i in prolinkdata2:\n",
    "\n",
    "    # New get requests for each link\n",
    "    page = requests.get(i)\n",
    "    \n",
    "    # Create a soup to parse the resuls from\n",
    "    Soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    \n",
    "    # Append the name attribute to the storedata dataset\n",
    "    prostoredata.append(Soup.find_all(\"link\", itemprop=\"name\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "b090278c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the stored channel name data in a csv format\n",
    "\n",
    "storedata = pd.DataFrame(prostoredata)\n",
    "\n",
    "storedata.to_csv('C:/Users/Gary/Documents/Thesis Files/CSV STORE/pro_unpacked_Youtube_links.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8574697",
   "metadata": {},
   "source": [
    "Now we will do the same process, but this time to extract the video title information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "9077eeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reset the storedata to empty.\n",
    "storedata = []\n",
    "linkdata2 = linkdata[\"Link\"]\n",
    "\n",
    "\n",
    "# Looping over the Youtube links\n",
    "for i in linkdata2:\n",
    "\n",
    "    # New get requests for each link\n",
    "    page = requests.get(i)\n",
    "    \n",
    "    # Create a soup to parse the resuls from\n",
    "    Soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    \n",
    "    # Append the title attribute to the storedata dataset\n",
    "    storedata.append(Soup.find_all(\"title\"))\n",
    "    \n",
    "#Reset the storedata to empty.\n",
    "prostoredata = []\n",
    "prolinkdata2 = prolinkdata[\"Link\"]\n",
    "\n",
    "# Looping over the Youtube links\n",
    "for i in prolinkdata2:\n",
    "\n",
    "    # New get requests for each link\n",
    "    page = requests.get(i)\n",
    "    \n",
    "    # Create a soup to parse the resuls from\n",
    "    Soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    \n",
    "    # Append the title attribute to the storedata dataset\n",
    "    prostoredata.append(Soup.find_all(\"title\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "860db95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the stored video title data in a csv format\n",
    "\n",
    "storedata = pd.DataFrame(storedata)\n",
    "\n",
    "storedata.to_csv('C:/Users/Gary/Documents/Thesis Files/CSV STORE/anti_unpacked_Youtube_titles.csv')\n",
    "\n",
    "prostoredata = pd.DataFrame(prostoredata)\n",
    "\n",
    "prostoredata.to_csv('C:/Users/Gary/Documents/Thesis Files/CSV STORE/pro_unpacked_Youtube_titles.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
